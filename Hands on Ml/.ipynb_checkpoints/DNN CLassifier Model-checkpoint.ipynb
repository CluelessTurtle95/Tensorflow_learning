{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator , ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNClassifier(BaseEstimator , ClassifierMixin):\n",
    "    def __init__(self , n_inputs , n_hidden , hidden_size , n_outputs, n_epoch , batch_size\n",
    "                 activation = tf.nn.elu  , optimizer = tf.train.AdamOptimizer , \n",
    "                 learning_rate = 0.001 , batch_norm = False , dropout = None):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.he_init = tf.variance_scaling_initializer()\n",
    "        \n",
    "        if self.batch_norm or dropout is not None :\n",
    "            self.training = tf.placeholder_with_default(False , shape=())\n",
    "    \n",
    "    def create_layer(self , inputs , index):\n",
    "        inputs = tf.layers.dense(inputs  , self.hidden_size , kernel_initializer=self.he_init , name=\"hidden{}\".format(index))\n",
    "        if self.batch_norm:\n",
    "            inputs = tf.layers.batch_normalization(inputs , momentum=0.9 , training=self.training , name=\"BN{}\".format(index))\n",
    "        inputs = self.activation(inputs , name=\"Activation{}\".format(index))\n",
    "        if self.dropout is not None :\n",
    "            inputs = tf.layers.dropout(inputs , dropout , training=self.training , name=\"Dropout{}\".format(index))\n",
    "        return inputs\n",
    "    \n",
    "    def next_batch(self)\n",
    "    \n",
    "    def fit(self , X , y):\n",
    "        ## building the model\n",
    "        Xp = tf.placeholder(tf.float32 , shape=(None , self.n_inputs) , name=\"X\")\n",
    "        yp = tf.placeholder(tf.int64 , shape=(None) , name=\"y\")\n",
    "        logits = Xp\n",
    "        for i in range(self.n_hidden):\n",
    "            logits = create_layer(logits , i + 1)\n",
    "        logits = tf.layers.dense(logits , self.n_outputs , activation=None , kernel_initializer=self.he_init)\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits , labels=yp)\n",
    "        \n",
    "        optimizer = self.optimizer(self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        \n",
    "        if self.batch_norm :\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            training_op = [training_op extra_update_ops]\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        n_batches = X.shape[0] // self.batch_size\n",
    "        \n",
    "        self.global_step = 0\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            for epoch in range(self.n_epoch):\n",
    "                for step in range(n_batches):\n",
    "                    self.global_step = step + epoch * n_batches\n",
    "                    if batch_norm or dropout is not None:\n",
    "                        feed_dict = {Xp : Xbatch , yp: ybatch , self.training:True}\n",
    "                    else :\n",
    "                        feed_dict = {Xp : Xbatch , yp: ybatch }\n",
    "                    sess.run(training_op , feed_dict=feed_dict)\n",
    "            \n",
    "            return\n",
    "    \n",
    "    def predict_proba(self , X):\n",
    "        pass\n",
    "    def predict(self , X):\n",
    "        pass\n",
    "    def get_model_params(self):\n",
    "        pass\n",
    "    def restore_model_params(self , params):\n",
    "        pass\n",
    "    def save(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
